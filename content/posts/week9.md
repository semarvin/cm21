---
title: "Week 9"
date: 2021-03-09T17:56:51-04:00
draft: false
---
Time: 4 1/2 hours

I downloaded the TopicModelingTool and dove right in. Immediately I noticed there was one issue that was skewing all of my results. The sizes (word counts) of all of my sources were dramatically different, so I would have to figure out a way to compare them on a more accurate scale. I discovered that the Documents tab shows the topics based on a percentage, rather than a word count, which is much better for me.

I ran the model in Corpus S the first time without a metadata .csv, but decided to add in a metadata .csv the second time and generate fifteen topics instead of ten. This offered a bit more of an insightful iteration. I later decided that I might gain some insight by running the samples I generated with SAIGE (a model of GPT-2 trained on Corpus S and made to write new text) through it instead. I wasn't having a lot of luck with TopicModelingTool and after looking over what my classmates generated, I switched to Voyant instead. I have a bit of experience with Voyant and decided to run the full Surrealist corpus against the samples generated by SAIGE, but that was not particularly illuminating. Given SAIGE's penchant for word reptition within a sample, it's hard to glean any intellectual insights when looking at word counts.