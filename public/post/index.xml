<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Ellie Marvin</title>
    <link>http://localhost:1313/post/</link>
    <description>Recent content in Posts on Ellie Marvin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 22 Mar 2021 17:56:51 -0400</lastBuildDate>
    
	<atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data Publication</title>
      <link>http://localhost:1313/post/week12/</link>
      <pubDate>Mon, 22 Mar 2021 17:56:51 -0400</pubDate>
      
      <guid>http://localhost:1313/post/week12/</guid>
      <description>For this week&#39;s exercise, I am sharing a few cleaned-up versions of texts generated by GPT-2. These .txt files have been linked to this page via GitHub and can be downloaded directly from GitHub.
Most of these texts came from NaNoGenMo is a twist on the popular NaNoWriMo&amp;ndash;or, National Novel Writing Month&amp;ndash;in which partcipants aim to generate 50,000 words of text using whatever coding, AI, or scripting they can. These texts and their generative codes are then gathered up and published on GitHub.</description>
    </item>
    
    <item>
      <title>Website Hosting</title>
      <link>http://localhost:1313/post/week10/</link>
      <pubDate>Mon, 15 Mar 2021 17:56:51 -0400</pubDate>
      
      <guid>http://localhost:1313/post/week10/</guid>
      <description>This week I focused on finally launching my site. I could not figure out how to connect my Jekyll site to my GitHub pages site, so I switched instead to Hugo. After many hours of trial and error, I finally figured out how to connect Hugo to Netlify via GitHub. After successfully deploying a site to host the samples generated by SAIGE, a model of GPT-2 trained on a corpus of Surrealist literature, I applied the same technique to a site for my Comp Methods GitHub repo.</description>
    </item>
    
    <item>
      <title>Topic Modeling</title>
      <link>http://localhost:1313/post/week9/</link>
      <pubDate>Tue, 09 Mar 2021 17:56:51 -0400</pubDate>
      
      <guid>http://localhost:1313/post/week9/</guid>
      <description>I downloaded the TopicModelingTool and dove right in. Immediately I noticed there was one issue that was skewing all of my results. The sizes (word counts) of all of my sources were dramatically different, so I would have to figure out a way to compare them on a more accurate scale. I discovered that the Documents tab shows the topics based on a percentage, rather than a word count, which is much better for me.</description>
    </item>
    
    <item>
      <title>Website Creation</title>
      <link>http://localhost:1313/post/week8/</link>
      <pubDate>Tue, 02 Mar 2021 17:56:51 -0400</pubDate>
      
      <guid>http://localhost:1313/post/week8/</guid>
      <description>I forgot to write something up for this exercise so I&#39;m going back retroactively. I was ill for most of lask week (got tested and it wasn&#39;t COVID, so it must have just been a horrible cold or the flu&amp;ndash;my roommate works at a hospital so who knows) but luckily had started working on this assignment the day after it was introduced. I&#39;ve worked with Hugo successfully in the past but not without difficulties and assumed (correctly) that Jekyll would be just as difficult.</description>
    </item>
    
    <item>
      <title>Data Visualization</title>
      <link>http://localhost:1313/post/week6/</link>
      <pubDate>Mon, 15 Feb 2021 17:56:51 -0400</pubDate>
      
      <guid>http://localhost:1313/post/week6/</guid>
      <description>Exercise 6/*! jQuery v1.11.3 | (c) 2005, 2015 jQuery Foundation, Inc. | jquery.org/license */!function(a,b){&#34;object&#34;==typeof module&amp;&amp;&#34;object&#34;==typeof module.exports?module.exports=a.document?b(a,!0):function(a){if(!a.document)throw new Error(&#34;jQuery requires a window with a document&#34;);return b(a)}:b(a)}(&#34;undefined&#34;!=typeof window?window:this,function(a,b){var c=[],d=c.slice,e=c.concat,f=c.push,g=c.indexOf,h={},i=h.toString,j=h.hasOwnProperty,k={},l=&#34;1.11.3&#34;,m=function(a,b){return new m.fn.init(a,b)},n=/^[\s\uFEFF\xA0]+|[\s\uFEFF\xA0]+$/g,o=/^-ms-/,p=/-([\da-z])/gi,q=function(a,b){return b.toUpperCase()};m.fn=m.prototype={jquery:l,constructor:m,selector:&#34;&#34;,length:0,toArray:function(){return d.call(this)},get:function(a){return null!=a?0a?this[a+this.length]:this[a]:d.call(this)},pushStack:function(a){var b=m.merge(this.constructor(),a);return b.prevObject=this,b.context=this.context,b},each:function(a,b){return m.each(this,a,b)},map:function(a){return this.pushStack(m.map(this,function(b,c){return a.call(b,c,b)}))},slice:function(){return this.pushStack(d.apply(this,arguments))},first:function(){return this.eq(0)},last:function(){return this.eq(-1)},eq:function(a){var b=this.length,c=+a+(0a?b:0);return this.pushStack(c=0&amp;&amp;bc?[this[c]]:[])},end:function(){return this.prevObject||this.constructor(null)},push:f,sort:c.sort,splice:c.splice},m.extend=m.fn.extend=function(){var a,b,c,d,e,f,g=arguments[0]||{},h=1,i=arguments.length,j=!1;for(&#34;boolean&#34;==typeof g&amp;&amp;(j=g,g=arguments[h]||{},h++),&#34;object&#34;==typeof g||m.isFunction(g)||(g={}),h===i&amp;&amp;(g=this,h--);ih;h++)if(null!=(e=arguments[h]))for(d in e)a=g[d],c=e[d],g!==c&amp;&amp;(j&amp;&amp;c&amp;&amp;(m.isPlainObject(c)||(b=m.isArray(c)))?(b?(b=!1,f=a&amp;&amp;m.isArray(a)?a:[]):f=a&amp;&amp;m.isPlainObject(a)?a:{},g[d]=m.extend(j,f,c)):void 0!==c&amp;&amp;(g[d]=c));return g},m.extend({expando:&#34;jQuery&#34;+(l+Math.random()).replace(/\D/g,&#34;&#34;),isReady:!0,error:function(a){throw new Error(a)},noop:function(){},isFunction:function(a){return&#34;function&#34;===m.type(a)},isArray:Array.isArray||function(a){return&#34;array&#34;===m.type(a)},isWindow:function(a){return null!=a&amp;&amp;a==a.window},isNumeric:function(a){return!m.isArray(a)&amp;&amp;a-parseFloat(a)+1=0},isEmptyObject:function(a){var b;for(b in a)return!1;return!0},isPlainObject:function(a){var b;if(!a||&#34;object&#34;!==m.type(a)||a.nodeType||m.isWindow(a))return!1;try{if(a.constructor&amp;&amp;!j.call(a,&#34;constructor&#34;)&amp;&amp;!j.call(a.constructor.prototype,&#34;isPrototypeOf&#34;))return!1}catch(c){return!1}if(k.ownLast)for(b in a)return j.call(a,b);for(b in a);return void 0===b||j.call(a,b)},type:function(a){return null==a?a+&#34;&#34;:&#34;object&#34;==typeof a||&#34;function&#34;==typeof a?</description>
    </item>
    
    <item>
      <title>Quantification</title>
      <link>http://localhost:1313/post/week4/</link>
      <pubDate>Tue, 02 Feb 2021 17:56:51 -0400</pubDate>
      
      <guid>http://localhost:1313/post/week4/</guid>
      <description>For this week&#39;s exercise, I used regular expressions to detect all of the named people in Corpus S. I&#39;ve noticed, while cleaning OCR, that Surrealist text seems to directly name many Surrealists and other influential figures. I wanted to see if that theory was true. I used the following regex:
1  ([A-Z][a-z]*) +([A-Z][a-z]*)  on RegExr to find all of the phrases that contained two capitalized words in each of the text files in Corpus S.</description>
    </item>
    
    <item>
      <title>Data Cleaning</title>
      <link>http://localhost:1313/post/week3/</link>
      <pubDate>Tue, 26 Jan 2021 17:56:51 -0400</pubDate>
      
      <guid>http://localhost:1313/post/week3/</guid>
      <description>All of the texts in Corpus AI and some of the texts in Corpus S were cleaned using regular expressions. Here&#39;s an example of my cleaning process:
Output from Tesseract:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  accordion.</description>
    </item>
    
    <item>
      <title>Corpus Selection</title>
      <link>http://localhost:1313/post/week2/</link>
      <pubDate>Tue, 19 Jan 2021 17:56:51 -0400</pubDate>
      
      <guid>http://localhost:1313/post/week2/</guid>
      <description>For this exercise, I chose to work with some data that will factor into the corpus I’m using to train GPT-2 for my Master’s project. The data is an excerpt from a book of poetry, Paul Eluard’s Letters to Gala. This text has a table at the beginning of it of the chronology of Eluard and Gala’s lives. Since the text is not in the public domain, I used scans from a copy of the text and ran them through Tesseract, an OCR software.</description>
    </item>
    
  </channel>
</rss>